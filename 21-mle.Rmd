# Ocenjivanje metodom maksimalne verodostojnosti

Jedan od najpoznatijih metoda za odredjivanje ocena parametra neke raspodele je upravo metod maksimalne verodostojnosti. On nalazi primenu u svim primenama statistike, od testiranja hipoteza do mašinskog učenja.

Osnovna postavka je sledeća:

Neka je $\mathbf{X}=(X_1, \dots, X_n)$ prost slučajan uzorak sa zajedničkom gustinom (ili zakonom raspodele) $f(\mathbf x;\theta)$ gde je $\theta$ nepoznati parametar. Funkcija verodostojnosti se definiše kao
 $$L(\theta; \mathbf x) = f(\mathbf x;\theta) = \prod_{i=1}^nf(x_i,\theta),$$ odnosno funkcija verodostojnosti je zajednička raspodela uzorka, posmatrana kao funkcija parametra $\theta$. Parametar može biti višedimenzioni (npr. $(\mu, \sigma^2)$ kod normalne raspodele), ali ćemo se mi baviti samo jednodimenzionim parametrima u ovom tekstu.
 
Ocena maksimalne verodostojnosti parametra $\theta$ je $$\hat{\theta} = \underset{\theta}{\mathrm{argmax}}L(\theta; \mathbf x).$$ Dakle ocena metodom maksimalne verodostojnosti je ona vrednosti parametra za koju je funkcija verodostojnosti najveća.

Da bismo našli ovaj maksimum, uglavnom prelazimo na traženje maksimuma logaritma funkcije verodostojnosti koji ima lepši oblik
$$l(\theta) = \log L(\theta, \mathbf x) = \sum_{i=1}^n\log f(x_i,\theta).$$
Maksimum dalje nalazimo odredjivanjem stacionarne tačke $l(\theta)$, rešavanjem jednačine $$\frac{d}{d\theta}l(\theta) = 0.$$ U velikom broju slučajeva rešenje ove jednačine, a samim tim i ocena, se može naći analitički, kao u primerima radjenim na matematičkoj statistici, ali nekada rešenje nije dostupno u analitičkom obliku.

U tom slučaju rešenje se mora naći numeričkim metodama. Jedan takav primer raspodele je Košijeva raspodela, koju ćemo koristiti za demonstraciju numeričkih metoda za nalaženje ocena maksimalne verodostojnosti.

## Njutnova metoda

Cilj Njutnove metode je da nadje nulu funkcije $f(x)$, tj. da nadje rešenje jednačine $$f(x) = 0.$$ Osnovna ideja metode je da se u svakoj iteraciji kriva $y=f(x)$ aproksimira njenom tangentom i nadje nula te tangente, čime se dobija neka aproksimacija nule početne funkcije, sve dok se ne dostigne neki uslov zaustavljanja.

Ako u $n$-toj iteraciji imamo aproksimaciju nule $x_n$, novu aproksimaciju $x_{n+1}$ dobijamo na sledeći način:

1. Nadjemo tangentu krive $y=f(x)$ u tački $x_n$, koja ima oblik
$$y_t = f'(x_n)(x-x_n)+f(x_n).$$
2. Aproksimacija u sledećoj iteraciji, $x_{n+1}$, je nula te tangente, odnosno rešenje jednačine $$0 = f'(x_n)(x-x_n)+f(x_n),$$ odakle dobijamo $$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}.$$

Pseudokod za Njutnovu metodu (nije u R-u!) izgleda ovako:

```
function njutn(x0, f, df, tol, maxiter) {
  x = x0 - f(x0) / df(x0)
  iter = 1
  
  while (abs(x - x0) > tol) {
    if(iter == maxiter)
      break
    x0 = x
    x = x0 - f(x0) / df(x0)
    iter = iter + 1
  }
  
  return x
}
```

### Primer Košijeve raspodele

Primenićemo Njutnovu metodu za nalaženje ocene maksimalne verodostojnosi parametra $\theta$ uzorka iz Košijeve raspodele sa gustinom

$$f(x, \theta)=\frac{1}{\pi}\frac{1}{1+(x-\theta)^2}.$$

Generisaćemo uzorak iz Košijeve raspodele za koji ćemo tražiti ocenu parametra.
```{r}
set.seed(21321)
xs <- rcauchy(1000, location = 10)
```

Logaritam funkcije verodostojnosti je $$l(\theta) = -\sum_{i=1}^n \log(1+(x_i-\theta)^2) - n\log\pi.$$

Definišimo je u R-u.
```{r}
logL <- function(teta, x) {
  - sum(log(1+(x-teta)^2)) - length(x)*log(pi)
}
```

Za naš uzorak ova funkcija izgleda ovako

```{r}
# Da bismo primenili funkciju curve, moramo prvo vektorizovati logL
vec_logL <- function(teta_vec, xs) {
  sapply(teta_vec, logL, xs)
}

curve(vec_logL(x, xs), xlim = c(-10, 30))
```

Izvod logaritma funkcije verodostojnosti je $$\frac{d}{d\theta}l(\theta) = \sum_{i=1}^n\frac{2(x_i-\theta)}{1+(x_i-\theta)^2}.$$

```{r}
dlogL <- function(teta, x) {
  sum(2*(x-teta) / (1+(x-teta)^2))
}
```

Izvod izgleda ovako
```{r}
vec_dlogL <- function(teta_vec, xs) {
  sapply(teta_vec, dlogL, xs)
}

curve(vec_dlogL(x, xs), xlim = c(0, 20))
```

A kako smo rekli da je ocena maksimalne verodostojnosti rešenje jednačine $\frac{d}{d\theta}l(\theta) = 0$, to znači da je ocena upravo nule nacrtane funkcije.

#### Njutnova metoda

Primenićemo Njutnovu metodu da nadjemo nulu funkcije $\frac{d}{d\theta}l(\theta)$.

Za to nam još treba izvod funkcije čiju nulu tražimo, a to je u našem slučaju drugi izvod logaritma funkcije verodostojnosti 

$$\frac{d^2}{d\theta^2}l(\theta) = -2\sum_{i=1}^n\frac{1-(x_i-\theta)^2}{(1+(x_i-\theta)^2)^2}.$$

```{r}
d2logL <- function(teta, x) {
  -2*sum((1-(x-teta)^2) / (1+(x-teta)^2)^2)
}
```

Ostaje još samo da implementiramo funkciju koja nalazi nulu funkcije Njutnovom metodom

```{r}
newton <- function(t0, f, df, tol=1e-6, maxiter=100, trace=FALSE, ...) {
  if (trace) print(t0)
  t <- t0 - f(t0, ...) / df(t0, ...)
  if (trace) print(t)
  iter <- 1
  while (abs(t - t0) > tol) {
    if(iter == maxiter)
      stop("Max iterations exceeded!")
    t0 <- t
    t <- t0 - f(t0, ...) / df(t0, ...)
    if (trace) print(t)
    iter <- iter + 1
  }
  return(t)
}
```


Na kraju, možemo odrediti ocenu metodom maksimalne verodostojnosti primenom Njutnove metode na izvod logaritma funkcije verodostojnosti

```{r, error=TRUE}
newton(0, dlogL, d2logL, x = xs, maxiter=100)
# da bi stampali iteracije, dodajte trace=TRUE
#newton(0, dlogL, d2logL, x = xs, maxiter=100, trace=TRUE)
```

Ali avaj! Imamo grešku, algoritam divergira, a vrednosti u iteracijama postaju sve veće.

Razlog za ovo je što smo loše odabrali početnu tačku 0. Njutnova metoda je vrlo osetljiva na odabir početne tačke, pa moramo uzeti neku smisleniju vrednost od koje da krenemo.

Možemo probati razne vrednosti dok ne dobijemo konvergenciju, a možemo i iskoristiti to što je u ovom slučaju $\theta$ parametar položaja, pa možemo pretpostaviti da je u blizini uzoračke sredine ili medijane, pa ćemo probati sa tim vrednostima

```{r, error=TRUE}
newton(mean(xs), dlogL, d2logL, x = xs, maxiter=100, trace=FALSE)
newton(median(xs), dlogL, d2logL, x = xs, maxiter=100, trace=TRUE)
```

Za prosek, koji je jednak 14.34 opet divergira algoritam, ali za medijanu koja je manje osetljiva na autlajere, kojih je mnogo zbog težine repova Košijeve raspodele, dobijamo konvergenciju.

Intuitivno, algoritam će u ovom slučaju konvergirati ako za početnu vrednost uzmemo neku vrednosti na padini na grafiku, oko tačke 10, što je prava vrednost parametra.

#### Fišerova modifikacija

Budući da znamo da je Fišerova informaciona funkcija za parametar $\theta$ jednaka $$\mathbf{I}(\theta) =-E\left(\frac{d^2}{d\theta^2}\log L(\theta; \mathbf{x})\right) = \frac n2,$$ možemo je iskoristiti (sa znakom minusa) umesto drugog izvoda logaritma funkcije verodostojnosti, čime dobijamo Fišerovu modifikaciju Njutnovog metoda. Dakle, osnovna ideja je da umesto drugog izvoda koristimo očekivanje drugog izvoda, čime "uprosečujemo" uticaj uzorka na drugi izvod, pa dobijamo funkciju samo od parametra.

```{r}
# Fiserov "drugi izvod"
fishd2logL <- function(teta, x) {
  - length(x) / 2
}

newton(median(xs), dlogL, fishd2logL, x = xs, maxiter=100, trace=TRUE)
```

Ako krenemo od medijane dobijamo istu vrednost kao za Njutnovu metodu. Medjutim, prednost Fišerove modifikacije je u tome što je manje osetljiva na odabir početne tačke. Na primer, Njutnova metoda je divergirala kad uzmemo prosek, ali Fišerova nalazi tačno rešenje:

```{r}
newton(mean(xs), dlogL, fishd2logL, x = xs, maxiter=100, trace=TRUE)
```

Fišerova metoda radi čak i ako krenemo od nule, koja je daleko od pravog rešenja.

```{r}
newton(0, dlogL, fishd2logL, x = xs, maxiter=100, trace=TRUE)
```

Možemo primetiti da imamo dosta više iteracija u Fišerovoj verziji, što je mana ove metode u odnosu na Njutnovu, koja brže konvergira (ukoliko konvergira).


## EM algoritam

EM algoritam (Expectation-Maximization) je algoritam za nalaženje ocena maksimalne verodostojnosti, koji nalazi veoma široku upotrebu u statistici, posebno Bajesovskoj. Glavna ideja je da nam omogući da nadjemo ocene maksimalne verodostojnosti u prisustvu nedostajućih podataka ili skrivenih parametara. 

Ilustrovaćemo postupak kroz dva primera.

### Ocena parametra sa histograma

Pretpostavimo da smo, umesto konkretnog uzorka, dobili samo histogram odredjenog uzorka iz eksponencijalne raspodele sa nepoznatim parametrom $\lambda$ koji želimo da ocenimo na osnovu tog histograma.

```{r}
set.seed(1)
# napravicemo histogram uzorka iz iz nepoznate eksponencijalne raspodele
h <- hist(rexp(1000, rate = rexp(1, 0.1)))
h
```

Budući da je logaritamska verodostojnost jednaka $$\log L(\lambda; \mathbf{x})=n\log\lambda - \lambda\sum_{i=1}^nx_i,$$
klasičnim postupkom možemo zaključiti da je ocena maksimalne verodostojnosti $$\hat{\lambda} = \frac{n}{\sum_{i=1}^nx_i}=\frac1{\overline{X}_n}.$$ Medjutim, naš problem je sada što mi nemamo uzorak $x_1,\dots,x_n$, pa ovako ne možemo odrediti ocenu. Ono što imamo je niz parova $(u_1,v_1),\dots,(u_n, v_n)$ koji odredjuju kom intevalu pripadaju odgovarajući elementi uzorka. Dakle, $x_i\in(u_i,v_i),\ \forall i=1,\dots,n.$

Srećom, na osnovu znanja o intervalu kom pripada $x_i$ možemo oceniti vrednost elementa $x_i$ uzimanjem uslovnog očekivanja, tačnije:
$$\hat{x_i} = E\left(x_i\mid x_i\in(u_i,v_i)\right)=\frac1\lambda + \frac{u_ie^{-\lambda u_i} - v_ie^{-\lambda v_i}}{e^{-\lambda u_i} - e^{-\lambda v_i}}.$$ Izvodjenje ovog uslovnog očekivanja ostavljamo čitaocu kao dobru vežbu za utvrdjivanje znanja iz Teorije verovatnoće.

Ovde opet imamo problem, sada nam nije poznato $\lambda$, pa ne možemo da odredimo tačnu ocenu elemenata uzorka. Vidimo da imamo "cirkularnu" zavisnost izmedju $\lambda$ i $x_i$, ako ne znamo jedno, ne možemo oceniti drugo.

Zato ćemo pribeći iterativnom algortmu.

1. Počećemo od neke proizvoljne ocene $\hat{\lambda}$.
2. (korak E) Iskoristićemo ovu ocenu da odredimo očekivanje $$\hat{x_i} = \frac{1}{\hat{\lambda}} + \frac{u_ie^{-\hat{\lambda} u_i} - v_ie^{-\hat{\lambda} v_i}}{e^{-\hat{\lambda} u_i} - e^{-\hat{\lambda} v_i}},\ i=1,\dots,n.$$
3. (korak M) Na osnovu ocena elemenata $x_i$, poboljšaćemo ocenu $\lambda$ maksimizovanjem funkcije verodostojnosi, tj. uzimamo $$\hat{\lambda} = \underset{\lambda}{\mathrm{argmax}}\log L(\lambda; \mathbf{\hat{x}})=\frac{n}{\sum_{i=1}^n\hat{x_i}}.$$
4. Ponavljamo korake E i M iterativno dok ne konvergiramo nekoj oceni nepoznatog parametra $\lambda$.

Vidimo odakle potiče ime EM algoritma -- smenjujemo korake E i M dok ne dobijemo dobru ocenu (ne maksimizujemo neko očekivanje).

Primenimo konačno ovaj algoritam da dobijemo ocenu za naš uzorak. Prvo ćemo implementirati algoritam i potrebne korake.

```{r}
# korak E - ocekivanja x-eva na osnovu neke cene lambda
expected_xs <- function(lambda, u, v) {
  1/lambda +
    (u*exp(-lambda*u) - v*exp(-lambda*v))/
    (exp(-lambda*u) - exp(-lambda*v))
}

# korak M - na osnovu ocenjenog uzorka, dati MMV ocenu lambda
maximize_logL <- function(xs) {
  1/mean(xs)
}


# funkcija koja trazi ocenu EM algoritmom za datu pocetnu vrednost
# lambda i vektor levih (u) i desnih (v) granica intervala za x_i
EM_estimate <- function(lambda_0, u, v, tol = 1e-8, maxiter = 1000) {
  xs <- expected_xs(lambda_0, u, v)
  lambda <- maximize_logL(xs)
  print(lambda)
  iter <- 1
  
  while((abs(lambda - lambda_0) > tol) &&
        iter < maxiter) {
    iter <- iter + 1
    lambda_0 <- lambda
    
    xs <- expected_xs(lambda_0, u, v)
    lambda <- maximize_logL(xs)
    print(lambda)
  }
  lambda
}
```

Ostaje jos da odredimo na osnovu histograma vrednosti $u_i$ i $v_i$. Nije bitno uredjenje uzorka, pa možemo pretpostaviti da imamo sortiran uzorak $x_i$. Odatle, ukoliko imamo 100 elemenata u prvom stupcu histograma, nekom intervalu $(a,b)$, onda možemo zaključiti da je $\forall i=1,\dots,100,\ x_i\in(u_i, v_i)=(a,b)$, odnosno da je $u_1=\dots=u_{100}=a$ i $v_1=\dots=v_{100}=b$. Slično važi i za ostale stupce. U R-u niz parova $(u_1,v_1),\dots,(u_n, v_n)$ možemo kreirati na sledeći način

```{r}
u <- rep(head(h$breaks, -1), h$counts)
v <- rep(tail(h$breaks, -1), h$counts)
```

Odavde sada možemo da odredimo ocenu $\lambda$ EM algoritmom:

```{r}
EM_estimate(1, u, v)
```

Zaključili smo da je ocena $\hat{\lambda] = 7.41$. Pošto ne znamo da li je ova ocena tačna, na neki način treba proveriti kako se ova ocena uklapa u podatke.

Najjednostavniji način je da preko histograma nacrtamo gustinu eksponencijalne raspodele sa ocenjenim parametrom.

```{r}
plot(h)
# mnozenje sa 100 je da bismo sa gustine presli na skalu frekvencija
# jer je density = counts / (sum(counts) * bin_width), sto je u nasem
# slucaju density = counts / (1000 * 0.1) = counts / 100
curve(100*dexp(x, rate = 7.41), add=TRUE, col="steelblue", lwd = 2)
```

Kriva vrlo dobro odgovara histogramu, pa možemo biti zadovljni ocenom.


